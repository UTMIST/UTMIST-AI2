{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T02:27:34.128673Z",
     "start_time": "2025-10-27T02:27:30.012570Z"
    }
   },
   "source": [
    "from environment.agent import Agent, UserInputAgent, run_real_time_match\n",
    "from environment.environment import WarehouseBrawl\n",
    "\n",
    "env = WarehouseBrawl()\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from typing import Optional\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kian/Projects/UTMIST-AI2/.venv1/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1] [1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 2, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 2, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1]\n",
      "Action space [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T02:27:35.640205Z",
     "start_time": "2025-10-27T02:27:35.636281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create replay memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "id": "f17ec9da7ab3394b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T02:27:36.244907Z",
     "start_time": "2025-10-27T02:27:36.240572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Deep Q-network\n",
    "class DQN_MLP_PFA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_MLP_PFA, self).__init__()\n",
    "        self.input_layer = nn.Linear(6900, 1024)\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            *[nn.Sequential(nn.Linear(1024, 1024), nn.ReLU()) for _ in range(10)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(1024, 32)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = self.hidden_layers(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Define DQN output to action-space function"
   ],
   "id": "9c02cc883e2ea229",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T02:27:37.452739Z",
     "start_time": "2025-10-27T02:27:36.723821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparams and epsilon-greedy policy\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.98\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 5000\n",
    "TAU = 0.001\n",
    "LR = 3e-4\n",
    "\n",
    "policy_net = DQN_MLP_PFA().to(device)\n",
    "target_net = DQN_MLP_PFA().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(30000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action_rep(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.choice(range(32))]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "def process_action_rep(action_rep):\n",
    "    # 0, 1%4, 2%4, 3%4, >16, 0, =(12-15, 28-31), =(4-7, 20-23), =(8-11, 24-27), 0\n",
    "    action_rep = action_rep.item()\n",
    "    return np.array([\n",
    "            0,\n",
    "            action_rep%4==1,\n",
    "            action_rep%4==2,\n",
    "            action_rep%4==3,\n",
    "            action_rep>16,\n",
    "            0,\n",
    "            12<=action_rep%16,\n",
    "            4<=action_rep%16<=7,\n",
    "            8<=action_rep%16<=11,\n",
    "            0\n",
    "            ])\n",
    "\n",
    "\n",
    "def process_half_obs(half_obs):\n",
    "    half_obs = torch.tensor(half_obs, device=device)\n",
    "    offset = 32\n",
    "    return torch.cat((half_obs[:8],\n",
    "               F.one_hot(half_obs[8].to(torch.int64), num_classes=13),\n",
    "               half_obs[9:13],\n",
    "               F.one_hot(half_obs[14].to(torch.int64), num_classes=13),\n",
    "               half_obs[offset:offset+8],\n",
    "               F.one_hot(half_obs[offset+8].to(torch.int64), num_classes=13),\n",
    "               half_obs[offset+9:offset+13],\n",
    "               F.one_hot(half_obs[offset+14].to(torch.int64), num_classes=13),\n",
    "               F.one_hot(half_obs[offset+15].to(torch.int64), num_classes=3),\n",
    "               half_obs[28:32]\n",
    "               ))"
   ],
   "id": "71aba90f90f3769f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T02:27:37.991094Z",
     "start_time": "2025-10-27T02:27:37.983926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define training step\n",
    "# Batch sample -> run policy -> compute TD error -> optimize\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(torch.float)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ],
   "id": "254c74836eaefd5d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T02:28:18.891496Z",
     "start_time": "2025-10-27T02:27:41.853111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate episodes\n",
    "\n",
    "num_episodes = 1\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    player_obs_list = []\n",
    "    opponent_obs_list = []\n",
    "    obs, info = env.reset()\n",
    "    player_obs_list += [torch.cat((process_half_obs(obs[0]), F.one_hot(torch.tensor(0, device=device), num_classes=32)))] * 60\n",
    "    opponent_obs_list += [torch.cat((process_half_obs(obs[1]), F.one_hot(torch.tensor(0, device=device), num_classes=32)))] * 60\n",
    "\n",
    "    player_state = torch.cat(player_obs_list).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "    for t in count():\n",
    "        player_action_rep = select_action_rep(player_state)\n",
    "        player_action = process_action_rep(player_action_rep)\n",
    "\n",
    "        opponent_action_rep = torch.tensor(0)\n",
    "        opponent_action = process_action_rep(opponent_action_rep)\n",
    "\n",
    "        full_action = {\n",
    "            0: player_action,\n",
    "            1: opponent_action\n",
    "        }\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(full_action)\n",
    "        print(reward)\n",
    "        # process reward... (env returns reward for both agents so player reward is reward[0])\n",
    "\n",
    "        reward = torch.tensor([reward[0]], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            player_obs_list = player_obs_list[1:] + [\n",
    "                torch.cat((\n",
    "                    process_half_obs(observation[0]),\n",
    "                    F.one_hot(player_action_rep.squeeze(), num_classes=32)\n",
    "                ))\n",
    "            ]\n",
    "            next_state = torch.cat(player_obs_list).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(player_state, player_action_rep, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            print(f\"{i_episode=}, {t=}\")\n",
    "            break\n",
    "\n",
    "print('Complete')"
   ],
   "id": "ef5a298a2749b1f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n",
      "{0: 0, 1: 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 56\u001B[39m\n\u001B[32m     53\u001B[39m state = next_state\n\u001B[32m     55\u001B[39m \u001B[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m \u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[38;5;66;03m# Soft update of the target network's weights\u001B[39;00m\n\u001B[32m     59\u001B[39m \u001B[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001B[39;00m\n\u001B[32m     60\u001B[39m target_net_state_dict = target_net.state_dict()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 30\u001B[39m, in \u001B[36moptimize_model\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Optimize the model\u001B[39;00m\n\u001B[32m     29\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# In-place gradient clipping\u001B[39;00m\n\u001B[32m     32\u001B[39m torch.nn.utils.clip_grad_value_(policy_net.parameters(), \u001B[32m100\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/UTMIST-AI2/.venv1/lib/python3.12/site-packages/torch/_tensor.py:521\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    511\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    512\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    513\u001B[39m         Tensor.backward,\n\u001B[32m    514\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    519\u001B[39m         inputs=inputs,\n\u001B[32m    520\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m521\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/UTMIST-AI2/.venv1/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    284\u001B[39m     retain_graph = create_graph\n\u001B[32m    286\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    287\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    288\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m289\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    290\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    291\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    292\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    293\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    294\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/UTMIST-AI2/.venv1/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    767\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    768\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m769\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    770\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    771\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    772\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    773\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T23:53:06.830845Z",
     "start_time": "2025-10-26T23:53:06.631707Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(policy_net.state_dict(), 'DQN_MLP_PFA3.pth')",
   "id": "2578f2b58ca2e414",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T23:54:34.613318Z",
     "start_time": "2025-10-26T23:54:34.603560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubmittedAgent(Agent):\n",
    "    def __init__(self, file_path: Optional[str] = None):\n",
    "        super().__init__(file_path)\n",
    "        self.past_obs = []\n",
    "        self.last_action_rep = 0\n",
    "\n",
    "\n",
    "    def _initialize(self) -> None:\n",
    "        self.model = DQN_MLP_PFA().to(device)\n",
    "        if self.file_path is not None:\n",
    "            self.model.load_state_dict(torch.load(self.file_path))\n",
    "\n",
    "\n",
    "    def predict(self, observation):\n",
    "        if len(self.past_obs) == 0:\n",
    "            self.past_obs = [torch.cat((process_half_obs(observation), F.one_hot(torch.tensor(0, device=device), num_classes=32)))] * 60\n",
    "        else:\n",
    "            self.past_obs = self.past_obs[1:] + [torch.cat((process_half_obs(observation), F.one_hot(torch.tensor(self.last_action_rep, device=device), num_classes=32)))]\n",
    "\n",
    "        state = torch.cat(self.past_obs).unsqueeze(0).to(torch.float32)\n",
    "        action_rep = self.model(state).max(1).indices.view(1, 1)\n",
    "        return process_action_rep(action_rep)\n",
    "\n"
   ],
   "id": "c9b28fe1689596f3",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T23:55:13.438539Z",
     "start_time": "2025-10-26T23:54:54.378460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_4 = SubmittedAgent(file_path=\"DQN_MLP_PFA3.pth\")\n",
    "agent_1 = UserInputAgent()\n",
    "max_timesteps = 30*90\n",
    "run_real_time_match(agent_1, agent_4)"
   ],
   "id": "d34e1dd9d53e6367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1] [1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 2, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 2, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1]\n",
      "Action space [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q3/95k_lwf54nzc0_kvmb_s3xs80000gn/T/ipykernel_55322/2802184954.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(self.file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground is rendered\n",
      "Ground is rendered\n",
      "Stage is rendered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.99it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 34.71it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 21.29it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 40.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MatchStats(match_time=10.766666666666667, player1=PlayerStats(damage_taken=0, damage_done=0, lives_left=2), player2=PlayerStats(damage_taken=0, damage_done=0, lives_left=0), player1_result=<Result.WIN: 1>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24f09c1a81ee770d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
