{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-26T21:27:37.923161Z",
     "start_time": "2025-10-26T21:27:37.560900Z"
    }
   },
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "from environment.agent import Agent\n",
    "from environment.environment import WarehouseBrawl\n",
    "\n",
    "env = WarehouseBrawl()\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1] [1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 2, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 2, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1]\n",
      "Action space [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:46:44.352770Z",
     "start_time": "2025-10-26T19:46:44.347886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create replay memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "id": "f17ec9da7ab3394b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:51:07.635901Z",
     "start_time": "2025-10-26T21:51:07.627962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Design Deep Q-network\n",
    "class DQN_MLP_PFA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_MLP_PFA, self).__init__()\n",
    "        self.input_layer = nn.Linear(6900, 1024)\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            *[nn.Sequential(nn.Linear(1024, 1024), nn.ReLU()) for _ in range(10)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(1024, 32)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = self.hidden_layers(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Define DQN output to action-space function"
   ],
   "id": "9c02cc883e2ea229",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:51:08.415481Z",
     "start_time": "2025-10-26T21:51:08.254205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparams and epsilon-greedy policy\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.98\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 5000\n",
    "TAU = 0.001\n",
    "LR = 3e-4\n",
    "\n",
    "policy_net = DQN_MLP_PFA()\n",
    "target_net = DQN_MLP_PFA()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(30000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action_rep(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.choice(range(32))]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "def process_action_rep(action_rep):\n",
    "    # 0, 1%4, 2%4, 3%4, >16, 0, =(12-15, 28-31), =(4-7, 20-23), =(8-11, 24-27), 0\n",
    "    action_rep = action_rep.item()\n",
    "    return np.array([0,\n",
    "            action_rep%4==1,\n",
    "            action_rep%4==2,\n",
    "            action_rep%4==3,\n",
    "            action_rep>16,\n",
    "            0,\n",
    "            12<=action_rep%16,\n",
    "            4<=action_rep%16<=7,\n",
    "            8<=action_rep%16<=11,\n",
    "            0\n",
    "            ])\n",
    "\n",
    "\n",
    "def process_half_obs(half_obs):\n",
    "    half_obs = torch.tensor(half_obs, device=device)\n",
    "    offset = 32\n",
    "    try:\n",
    "        return torch.cat((half_obs[:8],\n",
    "                   F.one_hot(half_obs[8].to(torch.int64), num_classes=13),\n",
    "                   half_obs[9:13],\n",
    "                   F.one_hot(half_obs[14].to(torch.int64), num_classes=13),\n",
    "                   half_obs[offset:offset+8],\n",
    "                   F.one_hot(half_obs[offset+8].to(torch.int64), num_classes=13),\n",
    "                   half_obs[offset+9:offset+13],\n",
    "                   F.one_hot(half_obs[offset+14].to(torch.int64), num_classes=13),\n",
    "                   F.one_hot(half_obs[offset+15].to(torch.int64), num_classes=3),\n",
    "                   half_obs[28:32]\n",
    "                   ))\n",
    "    except:\n",
    "        print(half_obs)\n",
    "        raise \"Error\""
   ],
   "id": "71aba90f90f3769f",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:52:25.833038Z",
     "start_time": "2025-10-26T21:52:25.825975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define training step\n",
    "# Batch sample -> run policy -> compute TD error -> optimize\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(torch.float)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ],
   "id": "254c74836eaefd5d",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:54:48.122490Z",
     "start_time": "2025-10-26T21:52:27.032012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate episodes\n",
    "'''\n",
    "Env is WarehouseBrawl\n",
    "- opponent\n",
    "- reward must be tuned\n",
    "'''\n",
    "\n",
    "num_episodes = 1\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    player_obs_list = []\n",
    "    opponent_obs_list = []\n",
    "    obs, info = env.reset()\n",
    "    player_obs_list += [torch.cat((process_half_obs(obs[0]), F.one_hot(torch.tensor(0), num_classes=32)))] * 60\n",
    "    opponent_obs_list += [torch.cat((process_half_obs(obs[1]), F.one_hot(torch.tensor(0), num_classes=32)))] * 60\n",
    "\n",
    "    player_state = torch.cat(player_obs_list).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "    for t in count():\n",
    "        player_action_rep = select_action_rep(player_state)\n",
    "        player_action = process_action_rep(player_action_rep)\n",
    "\n",
    "        opponent_action_rep = torch.tensor(0)\n",
    "        opponent_action = process_action_rep(opponent_action_rep)\n",
    "\n",
    "        full_action = {\n",
    "            0: player_action,\n",
    "            1: opponent_action\n",
    "        }\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(full_action)\n",
    "\n",
    "        # process reward... (env returns reward for both agents so player reward is reward[0])\n",
    "\n",
    "        reward = torch.tensor([reward[0]], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            player_obs_list = player_obs_list[1:] + [\n",
    "                torch.cat((\n",
    "                    process_half_obs(observation[0]),\n",
    "                    F.one_hot(player_action_rep.squeeze(), num_classes=32)\n",
    "                ))\n",
    "            ]\n",
    "            next_state = torch.cat(player_obs_list).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(player_state, player_action_rep, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            print(f\"{t=}\")\n",
    "            break\n",
    "\n",
    "print('Complete')"
   ],
   "id": "ef5a298a2749b1f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=1192\n",
      "Complete\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T20:46:22.666799Z",
     "start_time": "2025-10-26T20:46:22.400335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubmittedAgent(Agent):\n",
    "    def __init__(self, file_path=None):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "\n",
    "    def _process(self, obs):\n",
    "        obs = torch.tensor(obs, device=device)\n",
    "        offset = 32\n",
    "        return torch.cat((obs[:8],\n",
    "                   F.one_hot(obs[8].to(torch.int64), num_classes=13),\n",
    "                   obs[9:13],\n",
    "                   F.one_hot(obs[14].to(torch.int64), num_classes=13),\n",
    "                   obs[offset:offset+8],\n",
    "                   F.one_hot(obs[offset+8].to(torch.int64), num_classes=13),\n",
    "                   obs[offset+9:offset+13],\n",
    "                   F.one_hot(obs[offset+14].to(torch.int64), num_classes=12),\n",
    "                   F.one_hot(obs[offset+15].to(torch.int64), num_classes=3),\n",
    "                   obs[28:32]\n",
    "                   ))\n",
    "\n",
    "\n",
    "    def predict(self, obs):\n",
    "        if len(self.past_obs) == 0:\n",
    "            self.past_obs = [torch.cat((self._process(observation), F.one_hot(torch.tensor([0]), num_classes=32)))] * 60\n",
    "        else:\n",
    "            self.past_obs = self.past_obs[1:] + torch.cat((self._process(observation), F.one_hot(torch.tensor([0]), num_classes=32)))\n",
    "\n",
    "        # must process data and run model\n",
    "\n"
   ],
   "id": "c9b28fe1689596f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4620])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d34e1dd9d53e6367"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
